\section{Erläuterung der Projektschritte}
\subsection{Projektplanung und Management}

Angesichts der verschiedenen technologischen Komponenten und der Notwendigkeit einer koordinierten Teamarbeit war ein strukturiertes Projektmanagement für den Erfolg des Projekts zur Entwicklung eines Reddit-Sentiment-Analyse-Dashboards unerlässlich. Ziel war es, die Zusammenarbeit effizient zu gestalten, den Fortschritt transparent zu machen und die Einhaltung der Projektziele sicherzustellen. 

Zu Beginn des Projekts \textcolor{yellow}{haben wir uns/ wurde sich} zusammengesetzt und Brainstorming betrieben um auf eine Idee für ein Projekt zu kommen. Dabei wurden verschiedene Ansätze durchdacht und ChatGPT für präzisere Vorschläge verwendet. Problem dabei war es sich für eine Richtung zu entscheiden, \textcolor{yellow}{in die es gehen sollte}. Nachdem wir uns geeinigt haben, welches Projekt wir angehen wollen, ging es zügig voran mit dem Brainstorming. Dazu wurde, \textcolor{yellow}{wie in Bild x zu sehen}, ein erster Entwurf erstellt, indem die Ideen zur Visualisierung gesammelt worden sind. Dies diente der weiteren Planung, um das Projekt in die verschiedenen Hauptkomponenten aufteilen zu können. Damit die anstehenden Aufgaben und die fortschritte auch festgehalten werden können, wurde Notion als Projektmanagementtool verwendet. Notion ist eine webbasierte All-in-One-Software, die der Verwaltung von Informationen, die Funktion wie Notizen, Datenbanken, Projektmanagement und Kollaborationen in einer zentralen Oberfläche vereint. (Quelle GPT Beschreibung?!) Für dieses Projekt wurde insbesondere ein Kanban-Board verwendet, um eine übersichtliche Aufgabenverteilung und Fortschrittskontrolle zu gewährleisten. Dieses Board wurde nach klassischen agilen Prinzipien aufgebaut und umfasste Spalten wie „Backlog“, „To Do“, „In Progress“, „Review“ und „Done“. Jede Aufgabe wurde mit einer kurzen Beschreibung als Karte auf dem Board angelegt. Als nächstes wurden diese Priorisiert und die Aufgaben entsprechend aufgeteilt. Durch das verschieben der Karten konnte somit der Fortschritt der einzelnen Aufgaben jederzeit logisch nachhvollzogen werden und engpässe konnten rechtzeitig erkannt  und gemeinsam gelöst werden.

Zur gemeinsamen Entwicklung des Python Codes wurde die öffentliche Softwareentwicklungsplattform GitHub mit der Versionsverwaltungs-Software Git verwendet. Mit Hilfe dieser ist es leicht Änderungen am Code nachzuvollziehen, die verschiedenen Entwicklungsstände zu verwalten und Konflikte bei gleichzeitigem Arbeiten zu minimieren. GitHub wurrde somit als zentrales Repository verwendet, das als „Single Source of Truth“ für unseren Code diente und gleichzeitig als Backup fungierte. Um paralleles Arbeiten an verschiedenen Funktionalitäten (z.B. API-Anbindung vs. UI-Elemente) zu ermöglichen und die Stabilität des Haupt-Codes sicherzustellen, wurde ein Feature-Branch-Workflow verfolgt. Dabei wurde für jede neue Funktion oder größere Änderung ein eigener Branch erstellt. Nach Fertigstellung und interner Abstimmung wurde der Code des Branches über einen Merge-Prozess wieder in den Hauptentwicklungszweig integriert. Dieses Vorgehen förderte eine strukturierte Entwicklung und erleichterte die Integration der Beiträge aller Teammitglieder.

Ergänzend zu den genannten Tools fanden wöchentlich, meist Online, Teambesprechungen statt. Die Besprechungen dienten dem direkten Austausch der einzelnen Fortschritte der aufgeteilten Aufgaben, der klärung von Fragen, sowie der gemeinsamen Planung der nächsten Schritte. Somit konnte mit den bereits beschriebenen Tools eine effiziente Abstimmung gewährleistet werden. 

Zusammenfassend lässt sich sagen, dass die Kombination aus agiler Aufgabenverwaltung mit Notion, systematischer Versionskontrolle mit Git/GitHub für Code und Dokumentation sowie regelmäßiger Austausch eine solide Grundlage für die erfolgreiche Durchführung dieses Projekts bildete.

\subsection{Einsatz von Streamlit zur Visualisierung}

Für die Entwicklung der grafischen Benutzeroberfläche wurde Streamlit gewählt. Dabei handelt es sich um ein Open-Source-Framework in Python, das es ermöglicht, schnell und einfach interaktive Webanwendungen für Data-Science-Projekte zu erstellen. Die Entscheidung fiel auf Streamlit insbesondere aufgrund der einfachen Integration in den Python-Workflow. Zudem benötigt Streamlit keine zusätzliche Frontend-Programmierung in HTML, CSS und JavaScript. Dies erlaubt eine schnelle Entwicklung interaktiver Prototypen direkt aus Python heraus. 

Mit der Funktion \verb|@st.set_page_config()| wird in der main.py die Initialkonfiguration der Webanwendung vorgenommen. Sie wird am Anfang des Codes aufgerufen und ermöglicht es die grundlegenden Eigenschaften der Webanwendung festzulegen.
Ein weiteres wichtiges Element ist die durch \verb|@st.navigation()| implementierte Seitenstruktur. Diese erlaubt es, das Subreddit-Dashboard und das Posts-Dashboard als separate Seiten in die Anwendung zu integrieren. Durch diese modulare Struktur bleibt die Anwendung auch bei wachsender Funktionalität benutzerfreundlich und übersichtlich.
Entscheidend für die Performance der Streamlit-Anwendung ist der Einsatz mit den Caching-Funktionen \verb|@st.cache_data & st.cache_ressource|. Da Streamlit das gesamte Skript bei jeder Nutzerinteraktion potenziell neu ausführt, würden ohne Caching rechenintensive Operationen wie API-Abfragen oder das Laden von Machine-Learning-Modellen wiederholt durchgeführt. Somit werden die Caching-Funktionen von Streamlit in dem Projekt genutzt, um die Ergebnisse von Aufrufen zwischenzuspeichern. Dies verringert nicht nur die Latenz, sondern schont zudem die API-Rate-Limits von Reddit.

Im Gegensatz zu anderen Dashboardsystemen wie Dash oder Bokeh ist der initiale Konfigurationsaufwand bei Streamlit deutlich geringer, wodurch es sich besonders gut für dieses Projekt eignet. (zitat heaven2021) 
Ein weiterer Vorteil ist die stetig ansteigende Anzahl an Community-Komponenten und Drittanbieter-Integration, wodurch sich auch komplexere Visualisierungen (z.\,B.\ über Plotly oder WordCloud) nahtlos einbinden lassen, wie im weiteren Verlauf des Projekts zu sehen sein wird. 

Diese Merkmale machen Streamlit zu einem geeigneten Werkzeug für datengestützte Projekte, bei denen der Fokus auf schneller Umsetzung und einfacher Bedienbarkeit liegt.


\subsection{Datenanbindung \&~Vorbereitung}

\subsubsection{API-Grundlagen}
Eine REST-API (Representational State Transfer) ermöglicht
den strukturierten Abruf von Daten über das HTTP-Protokoll.
Dabei werden sogenannte HTTP-Verben wie \texttt{GET} oder \texttt{POST} verwendet, um Inhalte vom Server zu lesen oder neue zu senden.
Die meisten modernen APIs – darunter auch jene von Reddit – nutzen \texttt{JSON} als Austauschformat.
Ein API-Endpunkt wie beispielsweise \texttt{r/\{subreddit\}/comments} liefert die Kommentarstruktur eines Subreddits.
Jede Anfrage muss dabei autorisiert werden, meist über das OAuth2-Verfahren mit einer eindeutigen \texttt{CLIENT\_ID} und einem \texttt{CLIENT\_SECRET}.
Zusätzlich beschränken sogenannte Rate-Limits die Anzahl der Anfragen pro Zeiteinheit; bei zu vielen Zugriffen antwortet der Server mit einem Fehlercode wie \texttt{HTTP 429} („Too Many Requests“).

\subsubsection{PRAW: Python Reddit API Wrapper}
\texttt{PRAW} ist ein objektorientierter Wrapper für die Reddit-API, der die Kommunikation mit dem Server stark vereinfacht.
Anstatt manuell URLs zu erstellen und \texttt{JSON}-Antworten zu analysieren, arbeitet man direkt mit Python-Objekten wie \texttt{Subreddit}, \texttt{Submission} oder \texttt{Comment}.
Diese bieten intuitive Methoden wie \texttt{.top()} oder \texttt{.comments()}, um Inhalte gezielt zu durchsuchen.
Diese Abstraktion erhöht die Lesbarkeit des Codes erheblich, reduziert potenzielle Fehlerquellen und erlaubt einen deutlich schnelleren Entwicklungsfluss.
Zusätzlich ist \texttt{PRAW} aktiv gepflegt, umfassend dokumentiert und in der Community weit verbreitet.

\subsubsection{Authentifizierung \&~Umgebungsvariablen}
Zur sicheren Handhabung sensibler Zugangsdaten wird die Bibliothek \texttt{dotenv} verwendet.
Diese lädt Werte wie \texttt{CLIENT\_ID} und \texttt{CLIENT\_SECRET} aus einer externen Datei und schützt sie so vor versehentlicher Veröffentlichung im Code.
Die initiale Verbindung zur Reddit-API erfolgt über die Funktion \texttt{initialize\_reddit\_connection()}, die prüft, ob alle benötigten Informationen vollständig vorliegen.
Bei fehlenden Angaben wird eine aussagekräftige Fehlermeldung ausgegeben.

\subsubsection{Datenvorverarbeitung mit \texttt{data\_processor.py}}
Nach dem Abruf der Daten aus Reddit, \newline
beispielsweise mit der Funktion \texttt{get\_submissions\_text()}, liegen diese als einfache Zeichenketten vor.
Um die Texte analysierbar zu machen, erfolgt zunächst eine Bereinigung durch \texttt{clean\_up\_text()}, bei der Links, Benutzernamen und überflüssige Leerzeichen entfernt werden.
Die so vorbereiteten Daten durchlaufen anschließend eine Sentiment-Analyse \newline
mit Hilfe der \texttt{TextClassificationPipeline} von HuggingFace.
Diese weist jedem Beitrag ein Stimmungslabel zu.
Abschließend werden mit einem \texttt{Counter} die Häufigkeiten der jeweiligen Stimmungen aggregiert und für die Visualisierung im Dashboard vorbereitet.

\subsubsection{Caching \&~Session-State}
Um bei der Verwendung des Dashboards wiederholte API-Aufrufe und Rechenoperationen zu vermeiden, nutzt Streamlit zwei Formen des Cachings.
Langlebige Ressourcen wie die Reddit-Verbindung oder die Sentiment-Pipeline werden mit dem Dekorator \texttt{@st.cache\_resource} gespeichert.
Dynamischere Inhalte wie etwa die abgerufenen Beiträge oder Kommentare werden mit \texttt{@st.cache\_data} behandelt.
Darüber hinaus lassen sich mit dem Objekt \texttt{st.session\_state} benutzerspezifische Einstellungen, beispielsweise die gewählte Analysepipeline oder die Anzahl der Beiträge, dauerhaft über die Sitzung hinweg speichern.
Das erlaubt eine reibungslose und performante Nutzung interaktiver Komponenten im Dashboard.

\subsection{Testing}
Zur Sicherstellung der Funktionsfähigkeit und Wartbarkeit unseres Codes \newline
verfolgen wir eine zweigleisige Teststrategie: Einerseits werden \emph{Unit-Tests} eingesetzt, um einzelne Funktionen isoliert zu prüfen; andererseits ergänzen \emph{Integrationstests} die Testabdeckung durch das Zusammenspiel mehrerer Komponenten. Externe Abhängigkeiten wie die Reddit-API oder die Sentiment-Analyse-Pipeline von HuggingFace werden dabei mit Hilfe von \texttt{unittest.mock.MagicMock} simuliert, um stabile und deterministische Tests zu ermöglichen.

Beispielsweise \texttt{test\_reddit.py},\newline
wo Funktionen wie \texttt{get\_subreddit\_subscriber\_count()}\newline
und \texttt{get\_top\_submission\_url()} gegen einen simulierten Reddit-Zugriff getestet werden. Die Rückgabeobjekte wie \texttt{Submission} oder \texttt{Subreddit} werden dabei über Mocks erzeugt, sodass etwa die Methode \texttt{top()} eine Testsubmission liefert. Der Test prüft anschließend, ob die korrekte Abonnentenzahl oder eine gültige URL erzeugt wurde.

Auch die Vorverarbeitung wird getestet,\newline
wie etwa in \texttt{test\_sentiment\_analyzer.py}: Die Funktion \texttt{clean\_up\_text()} entfernt zuverlässig unerwünschte Bestandteile wie Links und Erwähnungen. Durch gezielte Eingabebeispiele wird das Verhalten dieser Funktion abgesichert.

Auf diese Weise stellen wir sicher, dass zentrale Datenflüsse und Analysebausteine unseres Dashboards auch bei Veränderungen im Code verlässlich funktionieren.
